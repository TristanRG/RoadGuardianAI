{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680778e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the needed imports and path\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "DATA_PATH = Path(\"../data/Motor_Vehicle_Collisions_-_Crashes.csv\")\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_inventory(df, top_n=8):\n",
    "    n = len(df)\n",
    "    rows = []\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        nn = int(ser.notna().sum())\n",
    "        null_rate = 1 - nn / n\n",
    "        nunique = int(ser.nunique(dropna=True))\n",
    "        top_vals = ser.dropna().value_counts().nlargest(top_n).to_dict()\n",
    "        rows.append({\n",
    "            \"column\": col,\n",
    "            \"dtype\": str(ser.dtype),\n",
    "            \"n_nonnull\": nn,\n",
    "            \"null_rate\": round(float(null_rate), 4),\n",
    "            \"nunique\": nunique,\n",
    "            \"top_vals_sample\": top_vals\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"null_rate\", ascending=False)\n",
    "\n",
    "sample = pd.read_csv(DATA_PATH, nrows=5000, low_memory=False)\n",
    "inv_df = column_inventory(sample)\n",
    "inv_df.head(50) \n",
    "inv_df.to_csv(OUT_DIR / \"column_inventory_sample.csv\", index=False)\n",
    "print(\"Inventory saved to:\", OUT_DIR / \"column_inventory_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be24631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cols we're interested in\n",
    "KEEP_COLS = [\n",
    "    \"CRASH DATE\", \"CRASH TIME\",\n",
    "    \"BOROUGH\", \"ZIP CODE\", \"LATITUDE\", \"LONGITUDE\", \"LOCATION\",\n",
    "    \"ON STREET NAME\", \"CROSS STREET NAME\", \"OFF STREET NAME\",\n",
    "    \"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\",\n",
    "    \"NUMBER OF PEDESTRIANS INJURED\", \"NUMBER OF PEDESTRIANS KILLED\",\n",
    "    \"NUMBER OF CYCLIST INJURED\", \"NUMBER OF CYCLIST KILLED\",\n",
    "    \"NUMBER OF MOTORIST INJURED\", \"NUMBER OF MOTORIST KILLED\",\n",
    "    \"CONTRIBUTING FACTOR VEHICLE 1\",\n",
    "    \"VEHICLE TYPE CODE 1\",\n",
    "    \"COLLISION_ID\"\n",
    "]\n",
    "\n",
    "sample_cols = pd.read_csv(DATA_PATH, nrows=2).columns.tolist()\n",
    "KEEP_COLS = [c for c in KEEP_COLS if c in sample_cols]\n",
    "print(\"Final keep columns ({}):\".format(len(KEEP_COLS)))\n",
    "for c in KEEP_COLS:\n",
    "    print(\" -\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, usecols=KEEP_COLS, low_memory=False)\n",
    "print(\"Loaded df shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# ZIP CODE to string  \n",
    "if \"ZIP CODE\" in df.columns:\n",
    "    df[\"ZIP CODE\"] = df[\"ZIP CODE\"].astype(str).str.strip().replace({'nan': None, 'None': None})\n",
    "\n",
    "# Ensuring numeric injury/killed columns are numeric \n",
    "num_cols = [c for c in df.columns if (\"NUMBER OF\" in c) or (\"KILLED\" in c) or (\"INJURED\" in c)]\n",
    "for col in num_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "print(\"Numeric columns cleaned:\", num_cols)\n",
    "df[num_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRASH DATE is MM/DD/YYYY in this dataset (US). Time is mostly 'H:MM' or 'HH:MM'.\n",
    "# Vectorized time normalization:\n",
    "time_raw = df[\"CRASH TIME\"].astype(str).str.strip().fillna(\"\")\n",
    "time_raw = time_raw.replace({\"nan\": \"\", \"NaN\": \"\", \"None\": \"\"})\n",
    "\n",
    "# Extract hh:mm where present\n",
    "hhmm = time_raw.str.extract(r'(?P<hhmm>\\d{1,2}:\\d{2})')['hhmm']\n",
    "\n",
    "mask_no_colon = hhmm.isna()\n",
    "digits = time_raw[mask_no_colon].str.extract(r'(\\d{1,4})')[0].fillna('')\n",
    "digits_padded = digits.str.zfill(4)\n",
    "formatted = digits_padded.str.slice(0,2) + ':' + digits_padded.str.slice(2,4)\n",
    "hhmm.loc[mask_no_colon] = formatted.values\n",
    "df[\"time_norm\"] = hhmm.fillna('').astype(str)\n",
    "\n",
    "# Parse date explicitly as MM/DD/YYYY\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"CRASH DATE\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Combine date + normalized time\n",
    "combined = df[\"date_parsed\"].dt.strftime(\"%Y-%m-%d\").fillna('') + ' ' + df[\"time_norm\"].fillna('')\n",
    "combined = combined.str.strip()\n",
    "df[\"ts\"] = pd.to_datetime(combined, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "# Report success rate\n",
    "total = len(df)\n",
    "parsed = df[\"ts\"].notna().sum()\n",
    "print(f\"Total rows: {total:,}\")\n",
    "print(f\"Parsed ts: {parsed:,}  ({parsed/total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_date_present_but_no_ts = df[\"CRASH DATE\"].notna() & df[\"ts\"].isna()\n",
    "print(\"Rows where CRASH DATE present but ts failed:\", mask_date_present_but_no_ts.sum())\n",
    "\n",
    "# Show a small sample of problematic pairs to see patterns\n",
    "display(df.loc[mask_date_present_but_no_ts, [\"CRASH DATE\", \"CRASH TIME\"]].drop_duplicates().head(40))\n",
    "\n",
    "# Rows with no CRASH DATE (rare?) — they cannot be used for time analysis\n",
    "print(\"Rows missing CRASH DATE entirely:\", df[\"CRASH DATE\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only rows with ts for time based analysis\n",
    "df_time = df[df[\"ts\"].notna()].copy()\n",
    "df_time[\"hour\"] = df_time[\"ts\"].dt.hour\n",
    "df_time[\"weekday\"] = df_time[\"ts\"].dt.dayofweek   \n",
    "df_time[\"is_weekend\"] = df_time[\"weekday\"].isin([5,6]).astype(int)\n",
    "df_time[\"date\"] = df_time[\"ts\"].dt.date\n",
    "\n",
    "df_time[\"severe\"] = (\n",
    "    (df_time[\"NUMBER OF PERSONS INJURED\"] > 0) |\n",
    "    (df_time[\"NUMBER OF PERSONS KILLED\"] > 0)\n",
    ").astype(int)\n",
    "\n",
    "print(\"df_time rows:\", len(df_time))\n",
    "df_time[[\"ts\",\"hour\",\"weekday\",\"severe\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = df_time.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).copy()\n",
    "\n",
    "zero_coords_mask = (df_geo[\"LATITUDE\"] == 0) | (df_geo[\"LONGITUDE\"] == 0)\n",
    "print(\"Rows with LAT or LON exactly 0:\", zero_coords_mask.sum())\n",
    "\n",
    "lat_min, lat_max = 40.4774, 40.9176\n",
    "lon_min, lon_max = -74.2591, -73.7004\n",
    "\n",
    "mask_valid_nyc = (\n",
    "    df_geo[\"LATITUDE\"].between(lat_min, lat_max) &\n",
    "    df_geo[\"LONGITUDE\"].between(lon_min, lon_max)\n",
    ")\n",
    "df_geo_clean = df_geo[mask_valid_nyc].copy()\n",
    "print(\"Rows with geo coords before filter:\", len(df_geo))\n",
    "print(\"Rows inside NYC bounding box:\", len(df_geo_clean))\n",
    "print(\"Rows removed:\", len(df_geo) - len(df_geo_clean))\n",
    "\n",
    "PRECISION = 4   \n",
    "df_geo_clean[\"lat_round\"] = df_geo_clean[\"LATITUDE\"].round(PRECISION)\n",
    "df_geo_clean[\"lon_round\"] = df_geo_clean[\"LONGITUDE\"].round(PRECISION)\n",
    "df_geo_clean[\"segment_id\"] = df_geo_clean[\"lat_round\"].astype(str) + \"_\" + df_geo_clean[\"lon_round\"].astype(str)\n",
    "\n",
    "print(\"Unique segments (approx):\", df_geo_clean[\"segment_id\"].nunique())\n",
    "df_geo_clean[[\"LATITUDE\",\"LONGITUDE\",\"segment_id\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0559a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by day, hour and week + day\n",
    "daily = df_time.set_index(\"ts\").resample(\"D\").size().rename(\"crash_count\").reset_index()\n",
    "\n",
    "hourly = df_time.groupby(\"hour\").size().reindex(range(24), fill_value=0)\n",
    "\n",
    "pivot = df_time.pivot_table(index=\"weekday\", columns=\"hour\", values=\"ts\", aggfunc=\"count\").fillna(0)\n",
    "\n",
    "# Useing cleaned data to compute top segments \n",
    "top_segments = df_geo_clean[\"segment_id\"].value_counts().nlargest(20)\n",
    "\n",
    "print(\"Daily rows:\", len(daily))\n",
    "print(\"Hourly counts sum (sanity):\", hourly.sum())\n",
    "print(\"Top segments (cleaned):\")\n",
    "display(top_segments.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for crashes per day\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(daily[\"ts\"], daily[\"crash_count\"], linewidth=1)\n",
    "plt.title(\"Crashes per day NYC — Time series\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Crash count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for crashes per hour\n",
    "\n",
    "hourly = df_time.groupby(\"hour\").size().reindex(range(24), fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(hourly.index, hourly.values, color='tab:red')\n",
    "plt.xticks(range(24))\n",
    "plt.xlabel(\"Hour of day\")\n",
    "plt.ylabel(\"Crash count\")\n",
    "plt.title(\"Crashes by hour of day\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week x hour heatmap\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.heatmap(pivot, cmap=\"Reds\", linewidths=0.2)\n",
    "plt.title(\"Crash counts: weekday (0=Mon) vs hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Weekday (0=Mon)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top segments\n",
    "\n",
    "top_segments = df_geo_clean[\"segment_id\"].value_counts().nlargest(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "y = range(len(top_segments))\n",
    "ax.barh(y, top_segments.values[::-1], color='tab:blue') \n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(top_segments.index[::-1])\n",
    "ax.set_xlabel(\"Crash count\")\n",
    "ax.set_title(\"Top 20 segments by crash count NYC filtered\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the cleaned data\n",
    "\n",
    "df_small = df_time.copy()\n",
    "\n",
    "if \"COLLISION_ID\" in df_small.columns and \"COLLISION_ID\" in df_geo_clean.columns:\n",
    "    df_small = df_small.merge(\n",
    "        df_geo_clean[[\"COLLISION_ID\", \"segment_id\"]],\n",
    "        how=\"left\",\n",
    "        on=\"COLLISION_ID\"\n",
    "    )\n",
    "\n",
    "cols_to_save = [\n",
    "    \"ts\",\"date\",\"hour\",\"weekday\",\"is_weekend\",\"severe\",\n",
    "    \"LATITUDE\",\"LONGITUDE\",\"segment_id\",\n",
    "    \"BOROUGH\",\"ZIP CODE\",\"ON STREET NAME\",\"CROSS STREET NAME\",\"COLLISION_ID\"\n",
    "]\n",
    "cols_to_save = [c for c in cols_to_save if c in df_small.columns]\n",
    "\n",
    "out_file = OUT_DIR / \"vehicle_collisions_subset.parquet\"\n",
    "\n",
    "try:\n",
    "    df_small[cols_to_save].to_parquet(out_file, index=False)\n",
    "    print(\"Saved cleaned subset to:\", out_file)\n",
    "except Exception as e:\n",
    "    print(\"Failed to write parquet. If the error mentions 'pyarrow' or 'fastparquet',\")\n",
    "    print(\"install pyarrow in your active conda env and re-run this cell:\")\n",
    "    print(\"  conda install -c conda-forge pyarrow -y\")\n",
    "    print(\"\\nFull error below:\\n\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roadguardianai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
