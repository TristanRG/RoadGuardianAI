{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0bc4068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet path: ..\\data\\processed\\vehicle_collisions_subset.parquet\n"
     ]
    }
   ],
   "source": [
    "#Imports & path\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil, sys\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PARQUET = OUT_DIR / \"vehicle_collisions_subset.parquet\"\n",
    "print(\"Parquet path:\", PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fbbb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows,cols: (2196754, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>severe</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-11 02:39:00</td>\n",
       "      <td>2021-09-11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WHITESTONE EXPRESSWAY</td>\n",
       "      <td>20 AVENUE</td>\n",
       "      <td>4455765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-26 11:45:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>QUEENSBORO BRIDGE UPPER</td>\n",
       "      <td>None</td>\n",
       "      <td>4513547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-01 01:29:00</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.62179</td>\n",
       "      <td>-73.970024</td>\n",
       "      <td>40.6218_-73.97</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11230</td>\n",
       "      <td>OCEAN PARKWAY</td>\n",
       "      <td>AVENUE K</td>\n",
       "      <td>4675373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-29 06:55:00</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>THROGS NECK BRIDGE</td>\n",
       "      <td>None</td>\n",
       "      <td>4541903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-21 13:21:00</td>\n",
       "      <td>2022-09-21</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BROOKLYN BRIDGE</td>\n",
       "      <td>None</td>\n",
       "      <td>4566131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ts        date  hour  weekday  is_weekend  severe  \\\n",
       "0 2021-09-11 02:39:00  2021-09-11     2        5           1       1   \n",
       "1 2022-03-26 11:45:00  2022-03-26    11        5           1       1   \n",
       "2 2023-11-01 01:29:00  2023-11-01     1        2           0       1   \n",
       "3 2022-06-29 06:55:00  2022-06-29     6        2           0       0   \n",
       "4 2022-09-21 13:21:00  2022-09-21    13        2           0       0   \n",
       "\n",
       "   LATITUDE  LONGITUDE      segment_id   BOROUGH ZIP CODE  \\\n",
       "0       NaN        NaN            None      None     None   \n",
       "1       NaN        NaN            None      None     None   \n",
       "2  40.62179 -73.970024  40.6218_-73.97  BROOKLYN    11230   \n",
       "3       NaN        NaN            None      None     None   \n",
       "4       NaN        NaN            None      None     None   \n",
       "\n",
       "            ON STREET NAME CROSS STREET NAME  COLLISION_ID  \n",
       "0    WHITESTONE EXPRESSWAY         20 AVENUE       4455765  \n",
       "1  QUEENSBORO BRIDGE UPPER              None       4513547  \n",
       "2            OCEAN PARKWAY          AVENUE K       4675373  \n",
       "3       THROGS NECK BRIDGE              None       4541903  \n",
       "4          BROOKLYN BRIDGE              None       4566131  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution (severe):\n",
      "severe\n",
      "0    0.758859\n",
      "1    0.241141\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Loading the cleaned data\n",
    "\n",
    "df = pd.read_parquet(PARQUET)\n",
    "print(\"Loaded rows,cols:\", df.shape)\n",
    "display(df.head())\n",
    "print(\"Target distribution (severe):\")\n",
    "print(df['severe'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94a78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ..\\data\\processed\\vehicle_collisions_subset.parquet\n",
      "Loaded rows,cols: (2196754, 14)\n",
      "Rows with geo coords before filter: 1956597 after: 1950503\n",
      "Unique segments (approx): 224155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>severe</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2103302</th>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.762127</td>\n",
       "      <td>-73.997387</td>\n",
       "      <td>40.7621_-73.9974</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>10036</td>\n",
       "      <td>11 AVENUE</td>\n",
       "      <td>WEST 44 STREET</td>\n",
       "      <td>37632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099258</th>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40.697753</td>\n",
       "      <td>-73.813916</td>\n",
       "      <td>40.6978_-73.8139</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2999940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102894</th>\n",
       "      <td>2012-07-01 00:10:00</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.588868</td>\n",
       "      <td>-73.972745</td>\n",
       "      <td>40.5889_-73.9727</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11223</td>\n",
       "      <td>WEST 3 STREET</td>\n",
       "      <td>BOUCK COURT</td>\n",
       "      <td>116256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101690</th>\n",
       "      <td>2012-07-01 00:10:00</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40.733610</td>\n",
       "      <td>-73.923840</td>\n",
       "      <td>40.7336_-73.9238</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3044659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099879</th>\n",
       "      <td>2012-07-01 00:20:00</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.677406</td>\n",
       "      <td>-73.983048</td>\n",
       "      <td>40.6774_-73.983</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11215</td>\n",
       "      <td>4 AVENUE</td>\n",
       "      <td>UNION STREET</td>\n",
       "      <td>175808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ts        date  hour  weekday  is_weekend  severe  \\\n",
       "2103302 2012-07-01 00:05:00  2012-07-01     0        6           1       0   \n",
       "2099258 2012-07-01 00:05:00  2012-07-01     0        6           1       1   \n",
       "2102894 2012-07-01 00:10:00  2012-07-01     0        6           1       0   \n",
       "2101690 2012-07-01 00:10:00  2012-07-01     0        6           1       1   \n",
       "2099879 2012-07-01 00:20:00  2012-07-01     0        6           1       0   \n",
       "\n",
       "          LATITUDE  LONGITUDE        segment_id    BOROUGH ZIP CODE  \\\n",
       "2103302  40.762127 -73.997387  40.7621_-73.9974  MANHATTAN    10036   \n",
       "2099258  40.697753 -73.813916  40.6978_-73.8139       None     None   \n",
       "2102894  40.588868 -73.972745  40.5889_-73.9727   BROOKLYN    11223   \n",
       "2101690  40.733610 -73.923840  40.7336_-73.9238       None     None   \n",
       "2099879  40.677406 -73.983048   40.6774_-73.983   BROOKLYN    11215   \n",
       "\n",
       "                           ON STREET NAME                 CROSS STREET NAME  \\\n",
       "2103302  11 AVENUE                         WEST 44 STREET                     \n",
       "2099258                              None                              None   \n",
       "2102894  WEST 3 STREET                     BOUCK COURT                        \n",
       "2101690                              None                              None   \n",
       "2099879  4 AVENUE                          UNION STREET                       \n",
       "\n",
       "         COLLISION_ID  \n",
       "2103302         37632  \n",
       "2099258       2999940  \n",
       "2102894        116256  \n",
       "2101690       3044659  \n",
       "2099879        175808  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR = Path(\"../data/processed\")\n",
    "PARQUET = OUT_DIR / \"vehicle_collisions_subset.parquet\"\n",
    "print(\"Loading\", PARQUET)\n",
    "df = pd.read_parquet(PARQUET)\n",
    "print(\"Loaded rows,cols:\", df.shape)\n",
    "\n",
    "df['ts'] = pd.to_datetime(df['ts'])\n",
    "\n",
    "df_time = df.sort_values('ts').copy()\n",
    "\n",
    "if 'LATITUDE' in df_time.columns and 'LONGITUDE' in df_time.columns:\n",
    "    df_geo = df_time.dropna(subset=['LATITUDE','LONGITUDE']).copy()\n",
    "else:\n",
    "    raise RuntimeError(\"LATITUDE / LONGITUDE columns missing in parquet. Re-run Day2 EDA to create them.\")\n",
    "\n",
    "lat_min, lat_max = 40.4774, 40.9176\n",
    "lon_min, lon_max = -74.2591, -73.7004\n",
    "mask_valid_nyc = (\n",
    "    df_geo['LATITUDE'].between(lat_min, lat_max) &\n",
    "    df_geo['LONGITUDE'].between(lon_min, lon_max)\n",
    ")\n",
    "df_geo_clean = df_geo[mask_valid_nyc].copy()\n",
    "print(\"Rows with geo coords before filter:\", len(df_geo), \"after:\", len(df_geo_clean))\n",
    "\n",
    "if 'segment_id' not in df_geo_clean.columns:\n",
    "    PRECISION = 4\n",
    "    df_geo_clean['lat_round'] = df_geo_clean['LATITUDE'].round(PRECISION)\n",
    "    df_geo_clean['lon_round'] = df_geo_clean['LONGITUDE'].round(PRECISION)\n",
    "    df_geo_clean['segment_id'] = df_geo_clean['lat_round'].astype(str) + \"_\" + df_geo_clean['lon_round'].astype(str)\n",
    "\n",
    "print(\"Unique segments (approx):\", df_geo_clean['segment_id'].nunique())\n",
    "df_geo_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2684e029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: ['hour', 'weekday', 'is_weekend', 'segment_id', 'BOROUGH']\n"
     ]
    }
   ],
   "source": [
    "# Ensure needed columns exist\n",
    "assert 'ts' in df.columns, \"ts missing; load the Day2 parquet instead.\"\n",
    "\n",
    "# ensure numeric features types\n",
    "df['hour'] = df['hour'].astype(int)\n",
    "df['weekday'] = df['weekday'].astype(int)\n",
    "df['is_weekend'] = df['is_weekend'].astype(int)\n",
    "\n",
    "if 'segment_id' not in df.columns:\n",
    "    PRECISION = 4\n",
    "    df['segment_id'] = (df['LATITUDE'].round(PRECISION).astype(str) + \"_\" +\n",
    "                        df['LONGITUDE'].round(PRECISION).astype(str))\n",
    "\n",
    "FEATURES = ['hour','weekday','is_weekend','segment_id','BOROUGH']\n",
    "TARGET = 'severe'\n",
    "\n",
    "df = df.dropna(subset=[TARGET]).copy()\n",
    "print(\"Using features:\", FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74af319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 1757403 Test rows: 439351\n",
      "Train date range: 2012-07-01 00:05:00 -> 2021-02-20 11:00:00\n",
      "Test  date range: 2021-02-20 11:08:00 -> 2025-08-05 23:30:00\n"
     ]
    }
   ],
   "source": [
    "# 80% train / 20% test by time order, sorted by ts\n",
    "df = df.sort_values('ts').reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(df))  \n",
    "train = df.iloc[:split_idx].copy()\n",
    "test  = df.iloc[split_idx:].copy()\n",
    "\n",
    "# Align ts to hourly window for history merge\n",
    "train['window_start'] = train['ts'].dt.floor('h')\n",
    "test['window_start']  = test['ts'].dt.floor('h')\n",
    "\n",
    "print(\"Train rows:\", len(train), \"Test rows:\", len(test))\n",
    "print(\"Train date range:\", train['ts'].min(), \"->\", train['ts'].max())\n",
    "print(\"Test  date range:\", test['ts'].min(), \"->\", test['ts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f43988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: segment_id\n",
      "  -> 5001 classes (train-encoded).\n",
      "Encoding: BOROUGH\n",
      "  -> 6 classes (train-encoded).\n",
      "X_train shape: (1757403, 5) X_test shape: (439351, 5)\n"
     ]
    }
   ],
   "source": [
    "cat_cols = [c for c in FEATURES if c not in ['hour','weekday','is_weekend']]\n",
    "encoders = {}\n",
    "\n",
    "TOP_K = 5000   \n",
    "high_cardinality_map = {\"segment_id\": TOP_K}  \n",
    "\n",
    "for c in cat_cols:\n",
    "    print(\"Encoding:\", c)\n",
    "    train_vals = train[c].fillna('UNK').astype(str)\n",
    "    if c in high_cardinality_map:\n",
    "        k = high_cardinality_map[c]\n",
    "        topk = set(train_vals.value_counts().nlargest(k).index)\n",
    "        train_reduced = train_vals.where(train_vals.isin(topk), other='OTHER')\n",
    "    else:\n",
    "        train_reduced = train_vals\n",
    "\n",
    "    unique_classes = train_reduced.unique().tolist()\n",
    "    if 'UNK' not in unique_classes:\n",
    "        unique_classes.append('UNK')\n",
    "    if 'OTHER' in unique_classes and 'OTHER' not in unique_classes:\n",
    "        unique_classes.append('OTHER')\n",
    "\n",
    "    mapping = {val: idx for idx, val in enumerate(unique_classes)}\n",
    "    encoders[c] = {\"mapping\": mapping, \"classes\": unique_classes}\n",
    "\n",
    "    train[c] = train_reduced.map(mapping).astype(int)\n",
    "\n",
    "    test_vals = test[c].fillna('UNK').astype(str)\n",
    "    if c in high_cardinality_map:\n",
    "        test_vals = test_vals.where(test_vals.isin(topk), other='OTHER')\n",
    "\n",
    "    test[c] = test_vals.map(mapping).fillna(mapping['UNK']).astype(int)\n",
    "\n",
    "    print(f\"  -> {len(mapping)} classes (train-encoded).\")\n",
    "\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "X_test  = test[FEATURES]\n",
    "y_test  = test[TARGET]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39247ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the LightGBM baseline\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols, free_raw_data=False)\n",
    "lgb_test  = lgb.Dataset(X_test,  label=y_test,  reference=lgb_train,  free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc','binary_logloss'],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], num_boost_round=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38575f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.5398054628011004\n",
      "PR-AUC (avg precision): 0.4423692475966587\n",
      "Precision@5%: 0.5227\n"
     ]
    }
   ],
   "source": [
    "#Evaluation metrics\n",
    "\n",
    "y_pred_prob = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "roc = roc_auc_score(y_test, y_pred_prob)\n",
    "pr  = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"ROC-AUC:\", roc)\n",
    "print(\"PR-AUC (avg precision):\", pr)\n",
    "\n",
    "k = 0.05\n",
    "n_top = max(1, int(k * len(y_pred_prob)))\n",
    "top_idx = np.argsort(y_pred_prob)[-n_top:]\n",
    "precision_at_k = y_test.iloc[top_idx].mean()\n",
    "print(f\"Precision@{int(k*100)}%: {precision_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc984720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>segment_id</td>\n",
       "      <td>465154.215250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hour</td>\n",
       "      <td>131476.012059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOROUGH</td>\n",
       "      <td>69651.276028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekday</td>\n",
       "      <td>20076.463642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>128.137260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     importance\n",
       "3  segment_id  465154.215250\n",
       "0        hour  131476.012059\n",
       "4     BOROUGH   69651.276028\n",
       "1     weekday   20076.463642\n",
       "2  is_weekend     128.137260"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imp = pd.DataFrame({\n",
    "    'feature': model.feature_name(),\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "display(imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e4b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model & encoders to ..\\models\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "joblib.dump(model, MODEL_DIR / \"lgb_baseline_model.joblib\")\n",
    "joblib.dump(encoders, MODEL_DIR / \"label_encoders.joblib\")\n",
    "print(\"Saved model & encoders to\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5158e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence in test: 0.4018017484881109 (40.18%)\n",
      "Model precision@5%: 0.5226931306049984\n",
      "Random baseline precision@k ~= prevalence: 0.4018017484881109\n",
      "Historical-seg precision@5%: 0.541403013611326\n"
     ]
    }
   ],
   "source": [
    "prevalence = y_test.mean()\n",
    "print(\"Prevalence in test:\", prevalence, f\"({prevalence:.2%})\")\n",
    "\n",
    "k = 0.05\n",
    "n_top = max(1, int(k * len(y_pred_prob)))\n",
    "top_idx = np.argsort(y_pred_prob)[-n_top:]\n",
    "model_prec_at_k = y_test.iloc[top_idx].mean()\n",
    "print(f\"Model precision@{int(k*100)}%:\", model_prec_at_k)\n",
    "\n",
    "print(\"Random baseline precision@k ~= prevalence:\", prevalence)\n",
    "\n",
    "hist_rate = train.groupby('segment_id')['severe'].mean()\n",
    "test_seg_rate = test['segment_id'].map(hist_rate).fillna(prevalence)\n",
    "top_idx_hist = np.argsort(test_seg_rate.values)[-n_top:]\n",
    "hist_prec_at_k = y_test.iloc[top_idx_hist].mean()\n",
    "print(f\"Historical-seg precision@{int(k*100)}%:\", hist_prec_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9598f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 1757403 Test rows: 439351\n",
      "Train range: 2012-07-01 00:05:00 -> 2021-02-20 11:00:00\n",
      "Test range:  2021-02-20 11:08:00 -> 2025-08-05 23:30:00\n"
     ]
    }
   ],
   "source": [
    "df_all = df_time.sort_values('ts').reset_index(drop=True)   \n",
    "split_idx = int(0.8 * len(df_all))\n",
    "train = df_all.iloc[:split_idx].copy()\n",
    "test  = df_all.iloc[split_idx:].copy()\n",
    "print(\"Train rows:\", len(train), \"Test rows:\", len(test))\n",
    "print(\"Train range:\", train['ts'].min(), \"->\", train['ts'].max())\n",
    "print(\"Test range: \", test['ts'].min(), \"->\", test['ts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f25caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History folder: ..\\data\\processed\\segment_history_files\n",
      "Files found: 0\n",
      "Total history size (GB): 0.0\n",
      "Example files (first 10):\n",
      "No history files found in folder. If you haven't run the writer, run the writer cell first.\n"
     ]
    }
   ],
   "source": [
    "HIST_DIR = Path(\"../data/processed/segment_history_files\")\n",
    "files = sorted(HIST_DIR.glob(\"*.parquet\"))\n",
    "print(\"History folder:\", HIST_DIR)\n",
    "print(\"Files found:\", len(files))\n",
    "\n",
    "total_bytes = sum(f.stat().st_size for f in files)\n",
    "print(\"Total history size (GB):\", round(total_bytes / 2**30, 3))\n",
    "print(\"Example files (first 10):\")\n",
    "for f in files[:10]:\n",
    "    print(\" -\", f.name, round(f.stat().st_size / 2**20, 3), \"MB\")\n",
    "\n",
    "# load a single file as a sanity check\n",
    "if files:\n",
    "    sample = pd.read_parquet(files[0])\n",
    "    display(sample.head())\n",
    "    print(\"Sample shape:\", sample.shape)\n",
    "else:\n",
    "    print(\"No history files found in folder. If you haven't run the writer, run the writer cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fb40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7267400f87094086837fd43434f52a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NEW_HIST_DIR = Path(\"../data/processed/segment_history_files_shifted_v2\")\n",
    "NEW_HIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOP_N = 5000\n",
    "top_segments = df_geo_clean['segment_id'].value_counts().nlargest(TOP_N).index.tolist()\n",
    "\n",
    "for seg in tqdm(top_segments):\n",
    "    seg_rows = df_geo_clean[df_geo_clean['segment_id'] == seg]\n",
    "    if seg_rows.empty:\n",
    "        continue\n",
    "    seg_ts = seg_rows.set_index('ts').sort_index()\n",
    "    start = seg_ts.index.min().floor('h')\n",
    "    end   = seg_ts.index.max().ceil('h')\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        continue\n",
    "    idx = pd.date_range(start=start, end=end, freq='h')\n",
    "\n",
    "    sev_hourly = seg_ts['severe'].resample('h').sum().reindex(idx, fill_value=0)\n",
    "    tot_hourly = seg_ts['severe'].resample('h').count().reindex(idx, fill_value=0)\n",
    "\n",
    "    sev_prev = sev_hourly.shift(1).fillna(0).astype(int)\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        'segment_id': [seg] * len(idx),\n",
    "        'sev_1h': sev_prev,\n",
    "        'sev_6h': sev_hourly.rolling(6).sum().fillna(0).astype(int),\n",
    "        'sev_24h': sev_hourly.rolling(24).sum().fillna(0).astype(int),\n",
    "        'tot_1h': tot_hourly.rolling(1).sum().fillna(0).astype(int),\n",
    "        'tot_6h': tot_hourly.rolling(6).sum().fillna(0).astype(int),\n",
    "        'tot_24h': tot_hourly.rolling(24).sum().fillna(0).astype(int),\n",
    "    }, index=idx)\n",
    "\n",
    "    tmp = tmp.reset_index().rename(columns={'index': 'window_start'})\n",
    "\n",
    "    safe_name = str(seg).replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\")[:200]\n",
    "    out_path = NEW_HIST_DIR / f\"{safe_name}.parquet\"\n",
    "    tmp.to_parquet(out_path, index=False, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f196ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_write_fast(df_split, out_parquet_path, hist_dir=HIST_DIR, hist_cols=None, batch_size=1000):\n",
    "    if hist_cols is None:\n",
    "        hist_cols = ['sev_1h','sev_6h','sev_24h','tot_1h','tot_6h','tot_24h']\n",
    "\n",
    "    df_split = df_split.copy()\n",
    "    df_split['segment_id'] = df_split['segment_id'].fillna('UNK').astype(str)\n",
    "    df_split['window_start'] = pd.to_datetime(df_split['ts']).dt.floor('h')\n",
    "\n",
    "    unique_segs = df_split['segment_id'].unique().tolist()\n",
    "    writer = None\n",
    "\n",
    "    for i in range(0, len(unique_segs), batch_size):\n",
    "        batch_segs = unique_segs[i:i+batch_size]\n",
    "        batch_df = df_split[df_split['segment_id'].isin(batch_segs)].copy()\n",
    "\n",
    "        hist_dfs = []\n",
    "        for seg in batch_segs:\n",
    "            safe_name = seg.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\")\n",
    "            hf_path = hist_dir / f\"{safe_name}.parquet\"\n",
    "            if hf_path.exists():\n",
    "                hf = pd.read_parquet(hf_path)\n",
    "                hf['segment_id'] = str(seg)\n",
    "                hf['window_start'] = pd.to_datetime(hf['window_start'])\n",
    "                hist_dfs.append(hf)\n",
    "\n",
    "        if hist_dfs:\n",
    "            hist_all = pd.concat(hist_dfs, ignore_index=True)\n",
    "            merged = batch_df.merge(hist_all, on=['segment_id', 'window_start'], how='left')\n",
    "        else:\n",
    "            merged = batch_df.copy()\n",
    "\n",
    "        for c in hist_cols:\n",
    "            if c not in merged.columns:\n",
    "                merged[c] = 0\n",
    "            merged[c] = merged[c].fillna(0).astype(int)\n",
    "        merged = merged.reset_index(drop=True)\n",
    "\n",
    "        table = pa.Table.from_pandas(merged, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(str(out_parquet_path), table.schema)\n",
    "        writer.write_table(table)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    else:\n",
    "        raise RuntimeError(\"No data written in merge_and_write_fast.\")\n",
    "\n",
    "\n",
    "OUT_MERGED = Path(\"../data/processed\")\n",
    "merge_and_write_fast(train, OUT_MERGED / \"train_with_hist_shifted_v2.parquet\")\n",
    "merge_and_write_fast(test,  OUT_MERGED / \"test_with_hist_shifted_v2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = pd.read_parquet(\"../data/processed/train_with_hist_shifted_v2.parquet\")\n",
    "test_s  = pd.read_parquet(\"../data/processed/test_with_hist_shifted_v2.parquet\")\n",
    "\n",
    "for df, name in [(train_s, \"train_shifted_v2\"), (test_s, \"test_shifted_v2\")]:\n",
    "    total = len(df)\n",
    "    n_sev1 = (df['sev_1h'] > 0).sum()\n",
    "    n_severe = df['severe'].sum()\n",
    "    prop = df.loc[df['sev_1h']>0, 'severe'].mean() if n_sev1>0 else np.nan\n",
    "    prop2 = df.loc[df['severe']==1, 'sev_1h'].gt(0).mean() if n_severe>0 else np.nan\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"rows:\", total)\n",
    "    print(\"rows with sev_1h>0:\", n_sev1, f\"({n_sev1/total:.2%})\")\n",
    "    print(\"rows with severe==1:\", n_severe, f\"({n_severe/total:.2%})\")\n",
    "    print(\"P(severe=1 | sev_1h>0):\", prop)\n",
    "    print(\"P(sev_1h>0 | severe=1):\", prop2)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63387d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../data/processed/train_with_hist_shifted_v2.parquet\")\n",
    "test  = pd.read_parquet(\"../data/processed/test_with_hist_shifted_v2.parquet\")\n",
    "\n",
    "target = \"severe\"\n",
    "drop_cols = [\"ts\",\"date\",\"segment_id\",\"BOROUGH\",\"ZIP CODE\",\"ON STREET NAME\",\"CROSS STREET NAME\",\"COLLISION_ID\",\"window_start\"]\n",
    "\n",
    "train[target] = train[target].astype(int)\n",
    "test[target]  = test[target].astype(int)\n",
    "\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_feats = [c for c in numeric_cols if c != target and c not in drop_cols]\n",
    "\n",
    "X_train = train[numeric_feats]\n",
    "y_train = train[target]\n",
    "X_test  = test[numeric_feats]\n",
    "y_test  = test[target]\n",
    "\n",
    "print(\"Using numeric features:\", numeric_feats)\n",
    "print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data  = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"binary_logloss\", \"auc\"],\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": -1,\n",
    "    \"verbose\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "callbacks = [lgb.callback.early_stopping(50), lgb.callback.log_evaluation(period=50)]\n",
    "\n",
    "bst = lgb.train(params, train_data, num_boost_round=1000, valid_sets=[train_data, test_data], callbacks=callbacks)\n",
    "\n",
    "best_iter = bst.best_iteration if getattr(bst, \"best_iteration\", None) else None\n",
    "y_pred_prob = bst.predict(X_test, num_iteration=best_iter)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1  = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "prevalence = y_test.mean()\n",
    "k = 0.05\n",
    "n_top = max(1, int(k * len(y_pred_prob)))\n",
    "top_idx = np.argsort(y_pred_prob)[-n_top:]\n",
    "model_prec_at_k = y_test.iloc[top_idx].mean()\n",
    "\n",
    "hist_rate = train.groupby('segment_id')['severe'].mean()\n",
    "test_seg_rate = test['segment_id'].map(hist_rate).fillna(prevalence)\n",
    "top_idx_hist = np.argsort(test_seg_rate.values)[-n_top:]\n",
    "hist_prec_at_k = y_test.iloc[top_idx_hist].mean()\n",
    "\n",
    "print(\"Prevalence (test):\", prevalence)\n",
    "print(f\"Model precision@{int(k*100)}%: {model_prec_at_k:.4f}\")\n",
    "print(f\"Historical-seg precision@{int(k*100)}%: {hist_prec_at_k:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "lgb.plot_importance(bst, max_num_features=20, importance_type=\"gain\", figsize=(10,6))\n",
    "plt.title(\"Top 20 Feature Importances (shifted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e031b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_j = pd.read_parquet(\"../data/processed/train_with_hist_shifted_v2.parquet\")\n",
    "test_j  = pd.read_parquet(\"../data/processed/test_with_hist_shifted_v2.parquet\")\n",
    "\n",
    "for df, name in [(train_j, \"train\"), (test_j, \"test\")]:\n",
    "    total = len(df)\n",
    "    n_sev1 = (df['sev_1h'] > 0).sum()\n",
    "    n_severe = df['severe'].sum()\n",
    "    prop = df.loc[df['sev_1h']>0, 'severe'].mean() if n_sev1>0 else np.nan\n",
    "    prop2 = df.loc[df['severe']==1, 'sev_1h'].gt(0).mean() if n_severe>0 else np.nan\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"rows:\", total)\n",
    "    print(\"rows with sev_1h>0:\", n_sev1, f\"({n_sev1/total:.2%})\")\n",
    "    print(\"rows with severe==1:\", n_severe, f\"({n_severe/total:.2%})\")\n",
    "    print(\"P(severe=1 | sev_1h>0):\", prop)\n",
    "    print(\"P(sev_1h>0 | severe=1):\", prop2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952be173",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../data/processed/train_with_hist_shifted_v2.parquet\")\n",
    "test  = pd.read_parquet(\"../data/processed/test_with_hist_shifted_v2.parquet\")\n",
    "\n",
    "train = train.sort_values(\"ts\").reset_index(drop=True)\n",
    "\n",
    "global_mean = train['severe'].mean()\n",
    "\n",
    "grp = train.groupby('segment_id')['severe']\n",
    "train['seg_count_so_far'] = grp.cumcount()\n",
    "train['seg_sum_so_far']   = grp.cumsum() - train['severe']           \n",
    "train['seg_mean_so_far']  = train['seg_sum_so_far'] / train['seg_count_so_far'].replace(0, np.nan)\n",
    "\n",
    "k = 10.0\n",
    "train['seg_te_smoothed'] = (train['seg_mean_so_far'] * train['seg_count_so_far'] + global_mean * k) / (train['seg_count_so_far'] + k)\n",
    "train['seg_te_smoothed'] = train['seg_te_smoothed'].fillna(global_mean)\n",
    "\n",
    "agg = train.groupby('segment_id').agg(n=('severe','count'), s=('severe','sum'))\n",
    "agg['mean'] = agg['s'] / agg['n']\n",
    "agg['te_smoothed'] = (agg['mean'] * agg['n'] + global_mean * k) / (agg['n'] + k)\n",
    "seg_te_map = agg['te_smoothed'].to_dict()\n",
    "\n",
    "train['seg_te'] = train['seg_te_smoothed']\n",
    "test['seg_te']  = test['segment_id'].map(seg_te_map).fillna(global_mean)\n",
    "\n",
    "train = train.drop(columns=['seg_count_so_far','seg_sum_so_far','seg_mean_so_far','seg_te_smoothed'])\n",
    "display(train[['segment_id','seg_te']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faff544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cyclical(df):\n",
    "    df['hour'] = pd.to_datetime(df['ts']).dt.hour\n",
    "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "    df['weekday'] = pd.to_datetime(df['ts']).dt.weekday\n",
    "    df['weekday_sin'] = np.sin(2*np.pi*df['weekday']/7)\n",
    "    df['weekday_cos'] = np.cos(2*np.pi*df['weekday']/7)\n",
    "    return df\n",
    "\n",
    "train = add_cyclical(train)\n",
    "test  = add_cyclical(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = train[['LATITUDE','LONGITUDE']].dropna()\n",
    "k = 200\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=10000)\n",
    "kmeans.fit(coords)\n",
    "train['cluster'] = kmeans.predict(train[['LATITUDE','LONGITUDE']].fillna(0))\n",
    "test['cluster']  = kmeans.predict(test[['LATITUDE','LONGITUDE']].fillna(0))\n",
    "\n",
    "cluster_agg = train.groupby('cluster')['severe'].agg(['count','mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "params = {\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\":\"auc\",\n",
    "    \"boosting_type\":\"gbdt\",\n",
    "    \"seed\":42,\n",
    "    \"verbosity\":-1\n",
    "}\n",
    "\n",
    "X = train[numeric_feats + ['seg_te']]\n",
    "y = train['severe'].astype(int)\n",
    "\n",
    "aucs = []\n",
    "for fold, (tr_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "    dtr = lgb.Dataset(X_tr, label=y_tr)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtr)\n",
    "    bst = lgb.train({**params, \"learning_rate\":0.05, \"num_leaves\":31},\n",
    "                    dtr, num_boost_round=500, valid_sets=[dval]\n",
    "                     )\n",
    "    yv = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "    auc_fold = roc_auc_score(y_val, yv)\n",
    "    print(\"Fold\", fold, \"AUC:\", auc_fold)\n",
    "    aucs.append(auc_fold)\n",
    "print(\"Mean CV AUC:\", np.mean(aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 0.05\n",
    "n_top = max(1, int(k * len(y_pred_prob)))\n",
    "top_idx = np.argsort(y_pred_prob)[-n_top:]\n",
    "print(\"Precision@5%:\", y_test.iloc[top_idx].mean())\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0,1],[0,1], linestyle='--')\n",
    "plt.xlabel(\"Predicted prob\")\n",
    "plt.ylabel(\"True prob\")\n",
    "plt.title(\"Calibration\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roadguardianai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
